- traditional storing of data
  - save a direct value
  - overrite this value with latest changes
  - there are can be a lot of changes, but what we have is a latet value
  - so we are losing all changes, all intents
- event source approach
  - we never change data
  - all changes are stored as a append only immutable event log
  - we never lose information
- difference
  - as a resulted state - these two representations are equal
  - but the second one contains all information and intents
  - let's imaging a case - on black friday day, a lot of people JUST BEFORE purchase
    deleted some items from a shopping cart - because a total prise was > then they planned
    nut basically all these items was very intersting for them, so let's find out all such cases
    and add 5% promotion and send out campaign emails
    in canonical way we can change our code to handle such cases, but the data already LOST
  - and there can be a lot of such cases, and we never know, which data we are losing today,
    but we really need it next year
  - event source has in-built protection agains data lost
- the on IMPORTANT difference
  - many teams are doing some sort of audit logs, we have - autlogs collection, hsitory collection
    even some kibana logs or it can be something different for this purpose, but ...
  - if we have application state - this is a final state of all our data, and it can be
    stored in different form in same db, in absolutly different db, in memory of running application
    or build on the fly on demand (per request) - and we have an event log, than in any given
    point of time, I can drop my application state and safely rebuild it from event log
  - and we should achieve this with a big confidence, because our
    application state is a first level derivative of the event log
  - if we can't do this for some reason - our system is not trully event-sourced

- code samples 01 / 02
  - simple read implementaton can be used for building read model
  - filter and sorting part should be done on database level
  - it is really simple code, you don't need any tools and frameworks for that

- why it exists
  - event sourcing is not new concept at all, maybe the term if fresh and fancy
    but the idea is really old
  - there are many different businesses which are already eventsourced by nature
    - finance - your bank account is not a just a number of your current balance
      it is a whole list of all debit/credit transactions, and only I will fold them
      all tohether I will have a balance. It's needed not only like a nice feature
      for customer of all his history records, but as a provement that his balance
      is correctly calcculated. in finance it was always like a append only mode,
      if you make a mistake thay never cross out this in their papaer, they fixed it
      with partial or full revert. Partial = +100 -> -10. Full = +100 -> -100 -> +90.
    - lawyer - your current working contract for example is not a single document.
      when there are some changes, they don't change existing terms, they ususally create
      Addendum -> addition. And if you need to understand you real final contract conditions
      you should take your original contract, all addendums in correct order, merge them
      and then you have an idea of you conditions.
    - health - doctor don't change you sick history, they always append all data to it
    - it
      - source control (git) - you current working copy is just a list of commits
        merged togehter in proper order
      - database (transactions) or oplog for syncing data between replicas
    and others
      - insurance
      - gambling

- projections
  - having a event log a very nice but there is one problem with this - doing different
    scans. For example just image that we have to do some queries against our data, it could
    be analytical queries like avg cart price per city, or some admin search - show all open
    carts created today, with failed payment. To do this we should have a final applicaiton
    state, to get which we have to rebuild all our aggregates, which can be time consuming
    if you have a lot of data. To solve this all event sourced systems use approach of
    creating read models (projection) based on event log.
  - read models are very powerfull
    - you can have as many of them as you want, if you compare them as alternative
      indeces in your db, they are partially cover such cases and indeces have drawbacks
      slow writes and not always fast enough (count in mongo) or heavy aggregations
      read models from other side are super fast, because they are focused on specific
      use cases and you have no restrictions.
    - they can be build on any technology - graph db, ealstic search which is more powerfull
      then mongodb full text search
    - there are two models two build projections server-driven and client-driven
    - server driven model based on using some message bus in between of event store and
      projection builders. When new event occurred we publish a new message into message
      bus which will notify all existing clients. There is a problem with this approach - how
      we can receive the whole history of events, when we adding new consumer or want to fully
      rebuild existing projection. For this we should introduce some communication between
      consumer and publisher - which is not really good, but still we have some problems with
      syncronization of processing old events and new events which can come at any time.
    - consumer drivern model works a little bit differently. evenry consumer keep track of
      his position, and publisher just implement a simple interface getEvents(from, limit).
      Then publisher has no idea of amount of consumers nad their activities, if they decide
      to rebuild their state or there 2 more consumer, publisher just doesn'care. For scaling
      here we can take a look into apache kafka.
    - a good example of consumer driven approach is a blog, there is a RSS/Atom and you can
      do whatever you want

- shanpshots
  - having a long living aggregates —Åan easily results in a many events.
    and when you need to load it, then upi can produce some performance issues. in order to solve this
    problem we can use snapshots. idea the following - you will create a fully serialized aggregate
    bound to specific event. then when you have to load this aggregate, you find a corresponding
    snapshots and all events created after snapshot was saved and apply only them.
  - we can create snapshots per N-events, or even calculate load time of aggregate, and if
    it will exceed some threshold  we create snapshot. It can be usefull if we applying events
    contains some heavy algorithmics calculation and amount of events is not proportial to time.
    The same we can apply by physical size of events
  - better to store snapshots separately not inside event log like
    a separate event, because this can lead to problem with versions. if we save them separately we
    can easily drop and recreate them, and keep our event stream append only.

- concurrency handling
  - if we have traditional saving model, we can solve concurrency problem by using update where
    modifiedAt or version equal to current. when no documents updated it means there is new version
    of aggregate exists. for event log we also can use version field with unique index on aggregateId
    and version, it will prevent inserting two events with same version.
  - in mongodb could be a problem with inserting many documents at once, the solution could
    be is to use transactions or don't use mongodb
  - you should also consider to generate snapshots with concurrency in mind.

- nice to know
  - learning curve - it exists but it is not harder then crud based, but because we all already
    have a lot of experience with crud we are thinking that it is easier, but basically they are
    really close to each other
  - GDPR - we can introduce separate map for our aggregate ids to some encryption key, then we just
    encode all our event paloads and snapshots with a corresponding key. When there is a request
    for "deletion" come, we just drop the entry inside map. On some basis we can do event stream
    compaction - where we can really delete items from event stream.
  - ocassionally conected - this become really popular on the web for PWA, when we can implement
    event sourcing on client app - pwa or native app, then users still can use partially our app
    and when connection is restored we can merge tow event streams, which is much easier then
    try to merge some calculated enteties. You can think about trello, wunderlist where concurrency
    between events can be easily solved in automatic way.

- when it hurts
  - unfamilliar - we are so experienced with CRUD and usually against of trying something new
  - versions - this is a most problematic thing, there is whole book from Greg Young about this,
    so basically you can start with simple event type  version field and have a separate handler
    for this event version, but with a tome it can produce more problem. better to get overview from
    book or have a separate talk.
  - eventual consistency - that is a little bit tricky because there no such constraints in
    event sourcing itself, but more practices force you to move to this direction, like read models.
    and thus can be challenging from point of view developers and business. i personally 5 years ago
    can not think about system that eventual consistent, but now we are really doing this with great
    level of success, i mean our queues. submit contract request, update non existing contract like
    a contract update, its just working fine.
  - external calls results - for example in item-added we have a price which of cause can change
    over the time, or some dynamic discounts we should capture thus inside event, in order to
    preserve same output on reprocessing of event log 
  - shared ids - if we for example on create event generate some id which then will be shared
    across other service ww should create a method which will produce same result, like hash
    functions or some predictable calculations or reprocessing or put the id inside an event.
    our action id is good example here.
  - extra storage - for sure it can be a problem, but now space is cheap and you should have
    really different data size to consider this as a problem

- when it shines
  - performance - projections with flexible persisten model give to us incredible read performance
    from one side, and append only event stream give huge performance on another. appending is much
    faster then locking+update or locking+delete
  - debugging
    - having a full event log give you really nice debugging capability.
      in case of a bug you can easily restore your aggregate to specific version, and then
      repeat an action. which produced a bug. then you can do a fix with full or partial recover.
      Partial recover it means you calculate difference between correct and wrong action and apply
      it, full is about fully revert an action and apply correct action. example bad transaction
      we sent 1000 eur instead of 100. partial fix is to get out 900. it can be more complex
      in reality because of several accounts involved or math is more harder so better to do
      full revert here. get out 1000, and then send out 100. with event sourcing is much easier
      to find and fix such cases.
  - tracing-audit - history items- a lot of work but still not precise and something is missing
    with es you out of the box have pretty good tool
  - flexibility - different read models as a graph DB or elasticsearch. We can scale read
    and writes separetly. We also can easily change internal structure of our aggregate,
    mandate type as a string, as a object, as a array of objects.
  - scalability - we can build as many projection as we want and scale read models separetly
    writes should perform fast enough. We can scale read and writes separetly.
  - no data lost - everything is tracked
  - maintanability
    - in many system we have generic methods and interfaces like a contract update.
      which is good from point of view less code, but which make the whole system to understand,
      because all logic is spread across all clients of your the service. with event sourcing you
      forced more to use use case driven design, where you have rich domain model with more clear
      methods like, brokerMandateRequested, publish, reject, transfer and etc.






