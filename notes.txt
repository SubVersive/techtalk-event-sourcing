- traditional storing of data
  - save a direct value
  - overrite this value with latest changes
  - there are can be a lot of changes, but what we have is a latet value
  - so we are losing all changes, all intents
- event source approach
  - we never change data
  - all changes are stored as a append only immutable event log
  - we never lose information
- difference
  - as a resulted state - these two representations are equal
  - but the second one contains all information and intents
  - let's imaging a case - on black friday day, a lot of people JUST BEFORE purchase
    deleted some items from a shopping cart - because a total prise was > then they planned
    nut basically all these items was very intersting for them, so let's find out all such cases
    and add 5% promotion and send out campaign emails
    in canonical way we can change our code to handle such cases, but the data already LOST
  - and there can be a lot of such cases, and we never know, which data we are losing today,
    but we really need it next year
  - event source has in-built protection agains data lost
- the on IMPORTANT difference
  - many teams are doing some sort of audit logs, we have - autlogs collection, hsitory collection
    even some kibana logs or it can be something different for this purpose, but ...
  - if we have application state - this is a final state of all our data, and it can be
    stored in different form in same db, in absolutly different db, in memory of running application
    or build on the fly on demand (per request) - and we have an event log, than in any given
    point of time, I can drop my application state and safely rebuild it from event log
  - and we should achieve this with a big confidence, because our
    application state is a first level derivative of the event log
  - if we can't do this for some reason - our system is not trully event-sourced

- code samples 01 / 02
  - simple read implementaton can be used for building read model
  - filter and sorting part should be done on database level
  - it is really simple code, you don't need any tools and frameworks for that

- why it exists
  - event sourcing is not new concept at all, maybe the term if fresh and fancy
    but the idea is really old
  - there are many different businesses which are already eventsourced by nature
    - finance - your bank account is not a just a number of your current balance
      it is a whole list of all debit/credit transactions, and only I will fold them
      all tohether I will have a balance. It's needed not only like a nice feature
      for customer of all his history records, but as a provement that his balance
      is correctly calcculated. in finance it was always like a append only mode,
      if you make a mistake thay never cross out this in their papaer, they fixed it
      with partial or full revert. Partial = +100 -> -10. Full = +100 -> -100 -> +90.
    - lawyer - your current working contract for example is not a single document.
      when there are some changes, they don't change existing terms, they ususally create
      Addendum -> addition. And if you need to understand you real final contract conditions
      you should take your original contract, all addendums in correct order, merge them
      and then you have an idea of you conditions.
    - health - doctor don't change you sick history, they always append all data to it
    - it
      - source control (git) - you current working copy is just a list of commits
        merged togehter in proper order
      - database (transactions) or oplog for syncing data between replicas
    and others
      - insurance
      - gambling

- projections
  - having a event log a very nice but there is one problem with this - doing different
    scans. For example just image that we have to do some queries against our data, it could
    be analytical queries like avg cart price per city, or some admin search - show all open
    carts created today, with failed payment. To do this we should have a final applicaiton
    state, to get which we have to rebuild all our aggregates, which can be time consuming
    if you have a lot of data. To solve this all event sourced systems use approach of
    creating read models (projection) based on event log.
  - read models are very powerfull
    - you can have as many of them as you want, if you compare them as alternative
      indeces in your db, they are partially cover such cases and indeces have drawbacks
      slow writes and not always fast enough (count in mongo) or heavy aggregations
      read models from other side are super fast, because they are focused on specific
      use cases and you have no restrictions.
    - they can be build on any technology - graph db, ealstic search which is more powerfull
      then mongodb full text search
    - there are two models two build projections server-driven and client-driven
    - server driven model based on using some message bus in between of event store and
      projection builders. When new event occurred we publish a new message into message
      bus which will notify all existing clients. There is a problem with this approach - how
      we can receive the whole history of events, when we adding new consumer or want to fully
      rebuild existing projection. For this we should introduce some communication between
      consumer and publisher - which is not really good, but still we have some problems with
      syncronization of processing old events and new events which can come at any time.
    - consumer drivern model works a little bit differently. evenry consumer keep track of
      his position, and publisher just implement a simple interface getEvents(from, limit).
      Then publisher has no idea of amount of consumers nad their activities, if they decide
      to rebuild their state or there 2 more consumer, publisher just doesn'care. For scaling
      here we can take a look into apache kafka.
    - a good example of consumer driven approach is a blog, there is a RSS/Atom and you can
      do whatever you want

- shanpshots
  - having a long living aggregates —Åan easily results in a many events.
    and when you need to load it, then upi can produce some performance issues. in order to solve this
    problem we can use snapshots. idea the following - you will create a fully serialized aggregate
    bound to specific event. then when you have to load this aggregate, you find a corresponding
    snapshots and all events created after snapshot was saved and apply only them.
  - we can create snapshots per N-events, or even calculate load time of aggregate, and if
    it will exceed some threshold  we create snapshot. It can be usefull if we applying events
    contains some heavy algorithmics calculation and amount of events is not proportial to time.
    The same we can apply by physical size of events
  - better to store snapshots separately not inside event log like
    a separate event, because this can lead to problem with versions. if we save them separately we
    can easily drop and recreate them, and keep our event stream append only.

- concurrency handling
  - if we have traditional saving model, we can solve concurrency problem by using update where
    modifiedAt or version equal to current. when no documents updated it means there is new version
    of aggregate exists. for event log we also can use version field with unique index on aggregateId
    and version, it will prevent inserting two events with same version.
  - in mongodb could be a problem with inserting many documents at once, the solution could
    be is to use transactions or don't use mongodb
  - you should also consider to generate snapshots with concurrency in mind.


